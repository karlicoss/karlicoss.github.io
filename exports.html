<!doctype html>
<html lang="en" prefix="og: http://ogp.me/ns#">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Icon made by Twitter -->
        <!-- https://twemoji.twitter.com/content/twemoji-twitter/en.html -->
        <link rel="icon" href="./robot-face.png">
        <link rel="apple-touch-icon" href="./robot-face.png">

        <meta name="generator" content="hakyll">
        <meta name="language" content="English">
        
        <meta name="keywords" content="infra dataliberation pkm">
        
        <!-- TODO concat with keywords tags; also need to make comma separated? -->

        <title>Building data liberation infrastructure | Mildly entertainingᵝ</title>

        <link href="https://fonts.googleapis.com/css?family=Source+Serif+Pro" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="./css/default.css?v=3" />
        <!-- TODO make conditional?? -->
        <link rel="stylesheet" href="./css/posts-list.css" /> 
        <link rel="stylesheet" href="./css/links.css?v=4" />

        

        <link rel="canonical" href="https://beepb00p.xyz/exports.html" />

        <!-- can test it with https://telegram.me/webpagebot -->
        <!-- or https://developers.facebook.com/tools/debug -->
        <meta property="og:type" content="website" />
        <meta property="og:url" content="https://beepb00p.xyz/exports.html" /> <!-- TODO base? -->
        <meta property="og:title" content="Building data liberation infrastructure | beepb00p" />
        <meta property="og:description" content="How to export, access and own your personal data with minimal effort" />
        <!-- ugh. why is image necessary??? otherwise other parts aren't working or I get lots of 404s -->
        <!-- ugh. Facebook really wants jpg? -->
        <!-- also Facebook displays it with black background, but whatever, fuck it. -->
        <meta property="og:image" content="https://beepb00p.xyz/robot-face.jpg" />

    </head>
    <body>
        <!-- TODO make semantic -->
        <header>
            <nav>
                <span class="nav-left">
                    <a class="fat" href="./">Home</a>
                    <!-- TODO eh, not sure if this symbol is good for that... -->
                    ·
                    <a class="fat" href="./ideas.html">Ideas</a>
                    ·
                    <a class="fat" href="./notes.html">Notes</a>
                    ·
                    <a class="fat" href="./tags.html">Tags</a>
                </span>
                <span class="nav-right">
                    <a class="fat" href="./feed.html">Feed</a>
                    ·
                    <a class="fat" href="./site.html">Site</a>
                    ·
                    <a class="fat" href="./me.html">Me</a>
                </span>
            </nav>
        </header>

        <main>
            

<!-- <link rel="stylesheet" href="/css/org.css" /> -->

<link rel="stylesheet" href="./css/htmlize.css" />
<link rel="stylesheet" href="./css/org-default.css" />

<link rel="stylesheet" href="./css/org-extra.css?v=3" />



<article>
    
    <div>THIS IS A DRAFT! It will appear on the main page once finished!</div>
    
    <section class="post-title">
    <h1>Building data liberation infrastructure</h1>
    <div class="summary">How to export, access and own your personal data with minimal effort</h2>
    </section>
    <!-- are sections appropriate for that? -->

    <section class="content">
    <p>
Our personal data is siloed, held hostage, and very hard to access for various technical and business reasons.
I wrote and vented a lot about it in the <a href="sad-infra.html">previous post</a>.
</p>
<p>
People suggest a whole spectrum of possible solutions to these issues, starting from proposals on dismantling capitalism and ending with high tech vaporwavy stuff like <a href="https://en.wikipedia.org/wiki/Urbit">urbit</a>.
</p>
<p>
I, however, want my data <b>here and now</b>. I'm also fortunate to be a software engineer so I can bring this closer to reality by myself.
</p>
<p>
As a pragmatic intermediate solution, feasible with existing technology and infrastructure without reinventing everything from scratch, 
I suggested a <a href="sad-infra.html#data_mirror"><b>'data mirror'</b></a>, a piece of software that continuously syncs/mirrors user's personal data.
</p>
<p>
So, as I promised, this post will be somewhat more <del>boring</del> specific.
</p>
<p>
You can treat this as a tutorial on liberating your data from any service. I'll be explaining some technical decisions and guidelines on:
</p>
<ul class="org-ul">
<li>how to reliably export your data from the cloud (and other silos), locally</li>
<li>how to organize it for easy and fast access</li>
<li>how to keep it up to date without constant maintenance</li>
<li>how to make it modular, so other people could use only parts they find necessary and extend it</li>
</ul>
<p>
In hindsight, some things feel so obvious, that they hardly deserve mention, but I hope that would be helpful anyway!
</p>
<p>
I will be presenting and elaborating on different technical decisions, patterns and tricks I figured out while developing data mirrors by myself.
</p>
<p>
I'm also <b>very open</b> to hear questions like "Why didn't you do Y instead of X?". 
It's quite possible that I'm slipping extra complexity somewhere and I would be very happy to eliminate it.
</p>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#design">1. Design principles</a></li>
<li><a href="#retrieve">2. Retrieving data</a>
<ul>
<li><a href="#api_public">public API</a></li>
<li><a href="#api_private">private API</a></li>
<li><a href="#export_scrape">scraping</a></li>
<li><a href="#export_manual">manual export (GDPR/takeout)</a></li>
<li><a href="#export_phone">phone apps</a></li>
<li><a href="#export_devices">devices</a></li>
</ul>
</li>
<li><a href="#types">3. Types of exports: a high-level view</a>
<ul>
<li><a href="#full">full export</a></li>
<li><a href="#incremental">incremental export</a></li>
<li><a href="#synthetic">synthetic export</a></li>
</ul>
</li>
<li><a href="#export_layer">4. Export layer</a>
<ul>
<li><a href="#bindings">use existing bindings</a></li>
<li><a href="#keep_raw">don't mess with the raw data</a></li>
<li><a href="#defensive">don't  be too defensive</a></li>
<li><a href="#credentials">allow reading credentials from a file</a></li>
</ul>
</li>
<li><a href="#storage">5. How to store it: organizing data</a>
<ul>
<li><a href="#timestamps">naming and timestamping</a></li>
<li><a href="#backups">backups</a></li>
<li><a href="#sync">synchronizing between computers</a></li>
<li><a href="#disk_space">disk space concerns</a></li>
</ul>
</li>
<li><a href="#dal">6. Data access layer (DAL)</a>
<ul>
<li><a href="#dal_performance">performance concerns</a></li>
<li><a href="#dal_examples">examples</a></li>
</ul>
</li>
<li><a href="#automatic_exports">7. Automating exports</a>
<ul>
<li><a href="#scheduling">scheduling</a></li>
<li><a href="#arctee">arctee</a></li>
</ul>
</li>
<li><a href="#fin">8. --</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org0000000">
<h2 id="design"><a class="headerlink" href="#design">¶</a><span class="section-number-2">1</span> Design principles</h2>
<div class="outline-text-2" id="text-design">
<p>
So just as a reminder: the idea of the data mirror is having personal data <b>continuously/periodically synchronized</b> to the file system, and having <b>programmatic access</b> to it.
</p>
<p>
That may be not that hard to achieve for one particular data source, but when you want to use <a href="my-data.html">ten or more</a>,
with each of them having its own quirks it becomes quite painful to write and support.
So there are many reasons to make it simple, generic, reliable and flexible at the same time, which isn't an easy goal.
</p>
<p>
The main principles of my design are modularity and separation of concerns. 
This allows making it easy to hook onto any layer to allow for different ways of using the data.
</p>
<div><span class="before-aside">
Most of my pipelines for data liberation consist of the following layers:

</span><aside class="sidenote">please don't be terrified of the word 'layer', typically these are just single scripts</aside></div>
<ul class="org-ul">
<li><p>
<a class="link-down" href="#export_layer">export layer</a>: knows how to get your data
</p>
<p>
The purpose of the export layer is to reliably fetch and serialize raw data on your disk. It roughly corresponds to the concept of the <a href="sad-infra.html#data_mirror">'data mirror app'</a>.
</p>
<p>
Export scripts deal with the tedious business of authorization, pagination, being tolerant of network errors, etc.
</p>
<p>
Example: the export layer for <a href="https://github.com/karlicoss/endoexport/blob/e322b44ca1e6e5a779b4e7ea49564ba60d425bfe/export.py#L10-L15">Endomondo data</a> 
is simply fetching exercise data from the API (using existing library bindings) and prints the JSON out. That's all it does.
</p>
<p>
In theory, this layer is the only essential one; merely having raw data on your disk enables you to use other tools to explore and analyze your data.
However, long term you'll find yourself doing the same manipulations all over again, and that's why we also need:
</p></li>
<li><p>
<a class="link-down" href="#dal">data access layer (DAL)</a>: knows how to read your data
</p>
<p>
For brevity, I'll refer to it as <b>DAL</b> (Data Abstraction/Access Layer).
</p>
<p>
The purpose of DAL is simply deserializing whatever the export script dumped and providing minimalistic data bindings.
It shouldn't worry about tokens, network errors, etc., once you have your data on the disk DAL should be able to handle it even when you're offline.
</p>
<p>
It's not meant to be too high level; otherwise, you might lose the generality and restring the bindings in such ways that they leave some users out.
</p>
<p>
I think it's very reasonable to keep export code and DAL code close, as you don't want serializing and deserializing to go out of sync, and that's what I'm doing in my export tools.
</p>
<div class="noop" id="dal_messenger">
<p>
Example: <a href="https://github.com/karlicoss/fbmessengerexport/blob/a8f65a259dfa36ab6d175461994356947ded142a/model.py#L27-L47">DAL for Facebook Messenger</a> knows how to read messages from the database, access certain fields (e.g. message body) and handle obscure details like converting timestamps to <samp class="inline">datetime</samp> objects. 
</p>
<ul class="org-ul">
<li>it's <b>not</b> trying to get messages from Facebook, which makes it way faster and more reliable to interact with data.</li>
<li>it's <b>not</b> trying to do anything fancier past providing access to the data, which allows to keep it simple and resilient</li>
</ul>
</div></li>
<li><p>
downstream data consumers
</p>
<p>
This is also what you could call the third layer (although the boundaries are not very well defined in this case).
</p>
<p>
As an input, it takes abstract (i.e. non-raw) data from DAL and actually does interesting things with it: analysis, visualizations, interactions across different data sources, etc.
</p>
<p>
For me, it's manifested as <a href="sad-infra.html#mypkg">a Python package</a>. I can simply import it in any Python script, and it knows how to read and access any of my data.
</p></li>
</ul>
<p>
Next, I'm going to elaborate on implementing the export layer.
</p>
</div>
</div>
<div class="outline-2" id="outline-container-org0000007">
<h2 id="retrieve"><a class="headerlink" href="#retrieve">¶</a><span class="section-number-2">2</span> Retrieving data</h2>
<div class="outline-text-2" id="text-retrieve">
<p>
The first step in exporting and liberating your data is to figure out what how are you actually supposed to fetch it?
</p>
<p>
I'll mostly refer to Python libraries (since that's what I'm mostly using and familiar with), but I'm quite sure there are analogs in other languages.
</p>
<p>
Also remember, this is just to fetch the data! If you get a file on the filesystem as a result, you can use any other programming language you like to access it.
That's the beauty of decoupling.
</p>
<p>
I won't elaborate much on what are the potential issues during the exports, as I wrote about it <a href="sad-infra.html#exports_are_hard">before</a>.
</p>
</div>
<div class="outline-3" id="outline-container-org0000001">
<h3 id="api_public"><a class="headerlink" href="#api_public">¶</a>public API</h3>
<div class="outline-text-3" id="text-api_public">
<p>
You register your app, authorize it, get a token, and you are free to call various endpoints and fetch whatever you want.   
</p>
<p>
I won't really elaborate on this, if you're reading this you probably have some idea how to use it.
Otherwise, I'm sure there are tutorials out there that would help you.
</p>
<p>
<span style="color:darkorange"><strong>if anyone knows decent ones, please let me know, I'll add links!</strong></span>
</p>
<p>
Examples: thankfully, most services out there offer public API to some extent
</p>
</div>
</div>
<div class="outline-3" id="outline-container-org0000002">
<h3 id="api_private"><a class="headerlink" href="#api_private">¶</a>private API</h3>
<div class="outline-text-3" id="text-api_private">
<p>
Sometimes service doesn't offer an API. 
But from the developer's perspective, it's still very reasonable to use one when you've got backend/frontend communication.
</p>
<p>
So chances are, the service just isn't exposing it, but you can spy on the token/cookies in your browser devtools and use them to access the API.
</p>
<p>
You can read more about handling such data sources here:
</p>
<ul class="org-ul">
<li><a href="https://willschenk.com/articles/2019/reverse_engineering_apis_using_chrome">Reverse engineering APIs using Chrome Developer Tools</a>: an extremely comprehensive and beginner-friendly tutorial</li>
<li><a href="https://www.freecodecamp.org/news/how-i-used-python-to-find-interesting-people-on-medium-be9261b924b0">"How I used Python to find interesting people to follow on Medium"</a>, an example of reverse engineering Medium API and using devtools</li>
<li><a href="https://blog.tendigi.com/starbucks-should-really-make-their-apis-public-6b64a1c2e923">Starbucks should really make their API public</a>: demo of reverse engineering Starbucks Android app, featuring using proxy and forging the signature</li>
</ul>
<p>
Some examples:
</p>
<ul class="org-ul">
<li>for <a href="https://github.com/karlicoss/fbmessengerexport">exporting Messenger data</a>, I'm using <a href="https://fbchat.readthedocs.io/en/stable">fbchat</a> library. It works by tricking Facebook into believing it's a browser and interacting with private API.</li>
<li>even though Pocket has an API, to get highlights from it, you need to <a href="https://github.com/karlicoss/pockexport#setting-up">spy on the API key</a> they use in the web app</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0000003">
<h3 id="export_scrape"><a class="headerlink" href="#export_scrape">¶</a>scraping</h3>
<div class="outline-text-3" id="text-export_scrape">
<p>
Sometimes service doesn't offer an API, doesn't use it even internally and serves HTML pages directly instead.
Or, reverse engineering API is so painful that scraping becomes a more feasible option.
</p>
<p>
Apart from the same difficulties you would have during API exports, there are some extras here:
</p>
<ul class="org-ul">
<li>authorization is harder: you definitely need username/password and potentially even 2FA token</li>
<li>DDOS protection: captchas, Cloudflare, etc.</li>
<li>or even deliberate anti-scraping measures</li>
</ul>
<p>
For Python the holy grail of scraping is <a href="https://scrapy.org">scrapy</a>:
</p>
<ul class="org-ul">
<li><a href="http://sangaline.com/post/advanced-web-scraping-tutorial">Advanced Web Scraping Tutorial</a>: bypassing "403 Forbidden", captchas, and more</li>
<li><a href="https://gist.github.com/alecxe/fc1527d6d9492b59c610">self-contained minimum example script to run scrapy</a></li>
</ul>
<p>
I'm pretty sure there are similar libraries for other languages, perhaps you could start with <a href="https://github.com/lorien/awesome-web-scraping">awesome-web-scraping repo</a> or <a href="https://news.ycombinator.com/item?id=15694118">Ask HN: What are best tools for web scraping?</a>.
</p>
<p>
For dealing with authorization, my personal experience is that using persistent <a href="https://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.firefox.firefox_profile">profile directory</a> in Selenium is sufficient in most cases: you can login once manually and after that, reuse the profile in your scripts.
</p>
<p>
Examples:
</p>
<ul class="org-ul">
<li>even though Hackernews has <a href="https://github.com/HackerNews/API">API for public data</a>, there is no way of getting your upvotes/saves without scraping HTML.</li>
<li>Amazon or Paypal have to be <a href="https://github.com/jbms/finance-dl#supported-data-sources">scraped</a> if you want your data.</li>
<li>my bank, HSBC doesn't have an API. Not that I expected it from HSBC, I don't live in a fairy tale; but even their manual transactions exports are in PDF which I have to <a href="https://github.com/karlicoss/hsbc-parser">parse</a>.</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0000004">
<h3 id="export_manual"><a class="headerlink" href="#export_manual">¶</a>manual export (GDPR/takeout)</h3>
<div class="outline-text-3" id="text-export_manual">
<p>
It's great they exist, and it is the easiest way to get your data if you just want a backup.
That doesn't really help in the long run though:
</p>
<ul class="org-ul">
<li>you can only do it now and then, sometimes there is a hard limit for the number of takeouts per year</li>
<li>it's manual: usually requires requesting and clicking on an email link</li>
<li>it's slow and asynchronous: normally takes a few days</li>
<li>the takeout format differs from the API format, often ends up as something neither machine friendly nor human friendly</li>
</ul>
<p>
That said, with a certain effort it can potentially be automated too.
</p>
<p>
They can be useful to get the 'initial' bit of your data, past the <a href="sad-infra.html#data_is_vanishing">API limits</a>.
</p>
<p>
Examples:
</p>
<ul class="org-ul">
<li><a href="https://takeout.google.com">Google Takeout</a></li>
<li><a href="https://help.twitter.com/en/managing-your-account/how-to-download-your-twitter-archive">Twitter Archive</a></li>
<li><a href="https://github.blog/2018-12-19-download-your-data">Github</a> data export</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0000005">
<h3 id="export_phone"><a class="headerlink" href="#export_phone">¶</a>phone apps</h3>
<div class="outline-text-3" id="text-export_phone">
<p>
I don't have an iPhone, so will only be referring to Android in this section, but I'd imagine the situation is similar.
</p>
<p>
These days, service might not even offer a desktop version at all and considering that scraping data off mobile apps is way harder, getting it from the phone directly might be an easier option. In addition, the data is often kept as an sqlite database, which is in many ways even better than using API!
</p>
<p>
On Android the story is simple: apps keep their data in <samp class="inline">/data/data/</samp> directory, which is not accessible unless you <b>root</b> your phone.
These days, with <a href="https://magiskmanager.com">magisk</a> it's considerably easier, however, it's still definitely not something a typical Android user would be able to do. Also rooting your phone can bring all sorts of trouble by triggering root detection (e.g. common in banking apps), so be careful.
</p>
<p>
Once you have root, you can write a script to copy necessary files from <samp class="inline">/data/data/</samp> to your target directory, synchronized with your computer (e.g. via <a href="https://play.google.com/store/apps/details?id=com.ttxapps.dropsync&amp;hl=en_GB">Dropbox</a> or <a href="https://play.google.com/store/apps/details?id=com.github.catfriend1.syncthingandroid&amp;hl=en_GB">Syncthing</a>).
</p>
<p>
Examples:
</p>
<ul class="org-ul">
<li>you can export Whatsapp data by copying <samp class="inline">/data/data/com.whatsapp/databases/msgstore.db</samp></li>
<li><a href="https://github.com/karlicoss/promnesia/blob/master/scripts/backup-phone-history.sh">scripts</a> for exporting mobile Chrome/Firefox browsing history</li>
<li>exporting <a href="https://bluemaestro.com">Bluemaestro</a> environment sensor data</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0000006">
<h3 id="export_devices"><a class="headerlink" href="#export_devices">¶</a>devices</h3>
<div class="outline-text-3" id="text-export_devices">
<p>
Here I'm referring to standalone specific-purpose devices like sleep trackers, e-ink readers, etc. The distinguishing thing is the device doesn't have Internet access or doesn't talk to any API.
</p>
<p>
You've got several options here:
</p>
<ul class="org-ul">
<li><p>
the device is capable of synchronizing with your phone (e.g. via Bluetooth)
</p>
<p>
It's probably easiest to rely on <a class="link-up" href="#export_phone">phone app exports</a> here.
If the sync has to be triggered manually, you can benefit from some <a href="https://play.google.com/store/apps/details?id=com.llamalab.automate&amp;hl=en">UI automation</a>.
</p></li>
<li><p>
the device is running Linux and has Internet access
</p>
<p>
That's often the case with e-ink readers.
</p>
<p>
You can potentially run the export script on the device itself and send the data somewhere else.
Another option is running an SSH server on the device and pulling data from it, but it's quite extreme.
</p></li>
<li><p>
the device can mount to a computer
</p>
<p>
Then, you can use <a href="https://en.wikipedia.org/wiki/Udev">udev</a> to trigger export when the device is plugged in.
If udev feels too complicated to you, even a cron script running every minute might be enough.
</p></li>
</ul>
<p>
Examples:
</p>
<ul class="org-ul">
<li>using <a href="https://github.com/karlicoss/kobuddy#as-a-backup-tool">kobuddy</a> for semiautomatic exports from Kobo e-ink reader</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0000012">
<h2 id="types"><a class="headerlink" href="#types">¶</a><span class="section-number-2">3</span> Types of exports: a high-level view</h2>
<div class="outline-text-2" id="text-types">
<p>
Hopefully, the previous section answers your questions on 'where do I get my data from'.
The next step is to figure out what do you actually need to request and how to store it.
</p>
<p>
Now, let's establish a bit of vocabulary here.
Since data exports by their nature are somewhat similar to <a href="https://en.wikipedia.org/wiki/Backup#Backup_methods">backups</a>, I'm borrowing some terminology.
</p>
<p>
The way I see it, there are three styles of data exports:
</p>
</div>
<div class="outline-3" id="outline-container-org000000a">
<h3 id="full"><a class="headerlink" href="#full">¶</a>full export</h3>
<div class="outline-text-3" id="text-full">
<p>
Every time you want your data, go exhaustively through all the endpoints and fetch the data.
As a result, you get some sort of JSON file, which you can save on the disk.
</p>
<p>
Typically, with full exports, it's okay to overwrite the older export once you fetched everything. 
</p>
</div>
<div class="outline-4" id="outline-container-org0000008">
<h4 id="org0000008">summary</h4>
<div class="outline-text-4" id="text-org0000008">
<ul class="org-ul">
<li>advantages
<ul class="org-ul">
<li>very straightforward to implement</li>
</ul></li>
<li>disadvantages
<ul class="org-ul">
<li><b>might be impossible</b> due to <a href="sad-infra.html#data_is_vanishing">API restrictions</a></li>
<li>takes <b>more resources</b>, i.e. time/bandwidth/CPU/memory</li>
<li>takes <b>more space</b> if you're keeping old versions</li>
<li>might be <b>flaky</b> due to excessive network requests</li>
</ul></li>
</ul>
</div>
</div>
<div class="outline-4" id="outline-container-org0000009">
<h4 id="org0000009">examples</h4>
<div class="outline-text-4" id="text-org0000009">
<p>
When would you use that kind of export?
When there isn't much data to retrieve and you can do it in one go.
</p>
<ul class="org-ul">
<li><p>
<a href="https://github.com/karlicoss/pockexport">Exporting Pocket data</a>
</p>
<p>
There are no apparent API limitations preventing from fetching everything, and it seems like a plausible option, presumably it's a matter of transferring a few hundred kilobytes. YMMV, if you are using it extremely heavily, you might want to use <a class="link-down" href="#synthetic">synthetic export</a>.
</p></li>
</ul>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org000000d">
<h3 id="incremental"><a class="headerlink" href="#incremental">¶</a>incremental export</h3>
<div class="outline-text-3" id="text-incremental">
<p>
'Incremental' means that rerunning export starts off the latest persisted state and only fetches missing data.
</p>
<p>
Implementation wise, it looks like this:
</p>
<ul class="org-ul">
<li>query previously exported data to determine the point (e.g. timestamp/message id) to continue from</li>
<li>fetch missing data starting from that point</li>
<li>merge it back with previously exported data, persist on disk</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org000000b">
<h4 id="org000000b">summary</h4>
<div class="outline-text-4" id="text-org000000b">
<ul class="org-ul">
<li>advantages
<ul class="org-ul">
<li>takes less resources</li>
<li>more resilient (if done right) as needs fewer network operations</li>
</ul></li>
<li>disadvantages
<ul class="org-ul">
<li>potentially very error-prone, harder to implement
<ul class="org-ul">
<li>if you're not careful with <a href="sad-infra.html#pagination">pagination</a> and misinterpret documentation you might never request some data</li>
<li>if you're not careful with <a href="sad-infra.html#consistency">transactional logic</a>, you might leave your export in an inconsistent state</li>
</ul></li>
</ul></li>
</ul>
<div><span class="before-aside">
Incremental exports are <b>always</b> harder to program. Indeed, <a class="link-up" href="#full">full export</a> is just an edge case of an incremental one.

</span><aside class="sidenote">Fun fact: most of your phone apps already implement incremental sync. It's a shame the logic can't be reused.</aside></div>
</div>
</div>
<div class="outline-4" id="outline-container-org000000c">
<h4 id="org000000c">examples</h4>
<div class="outline-text-4" id="text-org000000c">
<p>
So why would you bother with exporting data incrementally?
</p>
<ul class="org-ul">
<li><p>
too much data
</p>
<p>
This doesn't even mean too much in terms of bandwidth/storage, more of 'too many entities'.
</p>
<p>
E.g. imagine you want to export your Twitter timeline of 10000 tweets, that's about 1Mb of raw text data.
Even if you account for extra garbage and assume 10 Mb or even 100 Mb of data, it's basically nothing if you're running it once a day.
</p>
<p>
However, APIs usually impose pagination (e.g. 200 tweets per call), so to get these 10000 tweets you might have to do <code class="inline">10000 / 200 = 50</code> API calls. 
Suddenly the whole thing feels much less reliable, so you might want to make it incremental in order to minimize the number of network calls.
</p>
<p>
For example:
</p>
<ul class="org-ul">
<li><a href="https://github.com/fabianonline/telegram_backup">Telegram</a>/<a href="https://github.com/karlicoss/fbmessengerexport">Messenger</a>/Whatsapp – basically IM always means there's too much data to be exported at once</li>
</ul></li>
<li><p>
flaky/slow API
</p>
<p>
If it's the case you want to minimize network interaction.
</p>
<p>
For example:
</p>
<ul class="org-ul">
<li><a class="link-up" href="#export_scrape">web scraping</a> is always somewhat slow; in addition, you might have to rate limit yourself so you don't get banned by DDOS prevention.
Also, it's is even flakier than using APIs, so you might want to avoid extra work if possible.</li>
<li><a href="https://shop-eu.emfit.com/products/emfit-qs">Emfit QS</a> sleep data: API is a bit flaky, so I minimize network interaction by only fetching missing data.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org0000011">
<h3 id="synthetic"><a class="headerlink" href="#synthetic">¶</a>synthetic export</h3>
<div class="outline-text-3" id="text-synthetic">
<div><span class="before-aside">
This is a kind of a blend between full export and incremental export.   

</span><aside class="sidenote">if someone can think of a better term for describing this concept, please let me know!</aside></div>
<p>
It's similar to full export in the sense that there isn't that much data to retrieve and if you could, you would just fetch it in one go.
</p>
<p>
However, the difference that makes it similar to incremental exports is that you don't have all the data available at once, only the latest chunk.
The fundamental issue with synthetic exports is that no single export file will give you all of the data.
</p>
<p>
There are various reasons for that:
</p>
<ul class="org-ul">
<li><p>
API restrictions
</p>
<p>
Many APIs restrict the number of items you can retrieve through each endpoint for caching and performance reasons.
</p>
<p>
Example: <a href="https://github.com/karlicoss/rexport#limitations">Reddit</a> limits your API queries to 1000 entries.
</p></li>
<li><p>
Limited memory
</p>
<p>
Example: autonomous devices like HR monitors or temperature monitors are embedded systems with limited memory.
</p>
<p>
Typically, they use some kind of <a href="https://en.wikipedia.org/wiki/Circular_buffer">ring buffer</a> so when you export data, you only get, say, the latest 10000 measurements.
</p></li>
<li><p>
Disagreement on the 'state' of the system
</p>
<p>
Example: Kobo reader uses an <a href="https://github.com/karlicoss/kobuddy">sqlite database</a> for keeping metadata like highlights, which is awesome!
However, when you delete the book from your reader, it removes your annotations and highlights from the database too.
</p>
<p>
There is absolutely no reason to do this: I delete the book because I don't need it on my reader, not because I want to get rid of annotations.
So in order to have all of them my only option is having regular database snapshots and assembling the full database from these pieces.
</p></li>
<li><p>
Security
</p>
<p>
Example: <a href="https://docs.monzo.com/#list-transactions">Monzo bank API</a>. 
</p>
<blockquote>
<p>
After a user has authenticated, your client can fetch all of their transactions, and after 5 minutes, it can only sync the last 90 days of transactions. If you need the user’s entire transaction history, you should consider fetching and storing it right after authentication. 
</p>
</blockquote>
<p>
So that means that unless you're happy with manually authorizing every time you export, you will only have access to the last 90 days of transactions.
</p>
<p>
Note: I feel kind of sorry complaining at Monzo, considering they are the nicest guys out there in terms of being dev friendly.
But that's the only example of such behavior I've seen so far.
</p></li>
</ul>
<p>
One important difference from other types of exports is that you <b>have to</b> do them regularly/often enough.
Otherwise, you inevitably miss some data and in the best case, have to get it <a class="link-up" href="#export_manual">manually</a>, or in the worst case, <a href="./takeout-data-gone.html">lose it forever</a>.
</p>
<p>
Now, you could deal with these complications the same way you would with incremental exports, by pulling missing data only.
The <b>major difference</b> is that, if you do make a mistake in the logic, it's not just a matter of waiting to re-download everything. 
Some of the data might be gone <b>forever</b>.
</p>
<p>
So I take a hybrid approach instead:
</p>
<ul class="org-ul">
<li><p>
at <a class="link-down" href="#export_layer">export time</a>, retrieve all the data I can and keep it along with a timestamp, like <a class="link-up" href="#full">full export</a>.
</p>
<p>
Basically, it makes it an 'append-only system', so there is no opportunity for losing data.
</p></li>
<li><p>
at <a class="link-down" href="#dal">data access time</a>, we dynamically build (synthesize) full state of the data
</p>
<p>
We go through all exported data chunks and reconstruct the full state, similarly to <a class="link-up" href="#incremental">incremental export</a>.
That's where 'synthetic' comes from.
</p>
<p>
The 'full export' only exists at runtime, and errors in merging logic are not problematic as you never overwrite data.
If you do spot a problem, you only have to change the code, no need for data migrations.
</p></li>
</ul>
</div>
<div class="outline-4" id="outline-container-org000000e">
<h4 id="org000000e">illustrative example</h4>
<div class="outline-text-4" id="text-org000000e">
<p>
I feel like the explanations are still slightly confusing, so let's consider a specific scenario.
</p>
<p>
Say you've got a temperature sensor that takes a measurement every minute and keeps it in its internal database.
It's only got enough memory for 2000 datapoints so you have to grab data from it every day, otherwise, older measurements would be overwritten.
</p>
<p>
It seems like a perfect fit for synthetic export. 
</p>
<ul class="org-ul">
<li><p>
export layer: every day you run a script that connects to the sensor and copies the database onto your computer
</p>
<p>
That's it, it doesn't do anything more complicated than that.
It's an idempotent operation, so if Bluetooth connection fails, we can simply retry until we succeed without having to worry about transactional logic.
</p>
<p>
As a result, we get a bunch of files like:
</p>
<pre class="example">
# ls /data/temperature/*.db
...
20190715100026.db
20190716100138.db
20190717101651.db
20190718100118.db
20190719100701.db
...
</pre></li>
<li><p>
data access layer: go through all chunks and construct the full temperature history
</p>
<p>
E.g. it would look kind of like:
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">measurements</span>() -&gt; Iterator[<span class="org-builtin">float</span>]:
    processed: <span class="org-variable-name">Set</span>[datetime] = <span class="org-builtin">set</span>()
    <span class="org-keyword">for</span> db <span class="org-keyword">in</span> <span class="org-builtin">sorted</span>(Path(<span class="org-string">'/data/temperature'</span>).glob(<span class="org-string">'*.db'</span>)):
        <span class="org-keyword">for</span> timestamp, value <span class="org-keyword">in</span> query(db, <span class="org-string">'SELECT * FROM measurements'</span>):
            <span class="org-keyword">if</span> timestamp <span class="org-keyword">in</span> processed:
                <span class="org-keyword">continue</span>
            processed.add(timestamp)
            <span class="org-keyword">yield</span> value
</pre>
</div>
<p>
I hope it's clear how much easier is this than having some sort of master sqlite database and updating it.
</p></li>
</ul>
</div>
</div>
<div class="outline-4" id="outline-container-org000000f">
<h4 id="org000000f">summary</h4>
<div class="outline-text-4" id="text-org000000f">
<ul class="org-ul">
<li>advantages
<ul class="org-ul">
<li>much easier way to achieve incremental exports without having to worry about introducing inconsistencies</li>
<li><b>very resilient</b>, against pretty much everything: deleted content, data corruption, flaky APIs, programming errors</li>
<li><b>straightforward</b> to normalize and unify – you are not overwriting anything</li>
</ul></li>
<li>disadvantages
<ul class="org-ul">
<li><p>
takes <b>extra space</b>
</p>
<p>
That said, storage shouldn't be that much of a concern unless you export <b>very</b> often.
I elaborate on this problem <a class="link-down" href="#disk_space">later in the post</a>.
</p></li>
<li><p>
<b>overhead</b> at access time
</p>
<p>
When we access the data we have to merge all snapshots every time. I'll elaborate on this <a class="link-down" href="#dal_performance">later as well</a>.
</p></li>
</ul></li>
</ul>
</div>
</div>
<div class="outline-4" id="outline-container-org0000010">
<h4 id="org0000010">more examples</h4>
<div class="outline-text-4" id="text-org0000010">
<ul class="org-ul">
<li>Github API is restricted to 300 latest events, so synthetic logic is used in <a href="https://github.com/karlicoss/ghexport/blob/master/dal.py">ghexport</a> tool</li>
<li><p>
Reddit API is restricted to 1000 items, so synthetic logic is used in <a href="https://github.com/karlicoss/rexport/blob/874e6116bfba8cbd63fa3b4d93810a1488cb8464/dal.py#L136">rexport</a> tool
</p>
<p>
I elaborate on Reddit <a href="unnecessary-db.org#example_reddit">here</a>.
</p></li>
<li><p>
Chrome only keeps 90 days of browsing history in its database
</p>
<p>
<a href="unnecessary-db.org#chrome_dal">Here</a> I write in detail about why synthetic exports make a lot of sense for Chrome.
</p></li>
</ul>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0000018">
<h2 id="export_layer"><a class="headerlink" href="#export_layer">¶</a><span class="section-number-2">4</span> Export layer</h2>
<div class="outline-text-2" id="text-export_layer">
<p>
No matter which <a class="link-up" href="#types">of these</a> ways you have to use to export your data, there are some common difficulties, hence patterns that I'm going to explore in this section.
</p>
<p>
Just a quick reminder of <a href="sad-infra.html#exports_are_hard">the problems</a> that we're dealing with:
</p>
<ul class="org-ul">
<li>authorization: how to log in?</li>
<li>pagination: how to query the data correctly?</li>
<li>consistency: how to make sure we assemble the full view of data correctly without running into concurrency issues?</li>
<li>rate limits: how to respect the service's policies and not get banned?</li>
<li>error handling: how to be defensive enough without making the code too complicated?</li>
</ul>
<p>
My guiding principle is: during the export, do the <b>absolute minimum</b> work required to reliably get raw data on your disk.
This is kind of vague (perhaps even obvious), so I will try to elaborate on what I mean by that.
</p>
<p>
This section doesn't cover the exact details, it's more of a collection of tips for minimizing the work and boilerplate. If you are interested in reading the code, <a href="https://github.com/search?type=Repositories&amp;q=user%3Akarlicoss+++topic%3Aexport">here</a> are some of the export scripts and tools I've implemented.
</p>
</div>
<div class="outline-3" id="outline-container-org0000014">
<h3 id="bindings"><a class="headerlink" href="#bindings">¶</a>use existing bindings</h3>
<div class="outline-text-3" id="text-bindings">
<p>
This may be obvious, but I still feel it has to be said.
Unless retrieving data is trivial (i.e. single GET request), chances that someone has already invested effort in dealing with various API quirks.
Bindings often deal with dirty details like rate limiting, retrying, pagination, etc. So if you're lucky you might end up spending very little effort on actually exporting data.
</p>
<p>
If there is something in bindings you don't like or lack, it's still easier to <a href="https://en.wikipedia.org/wiki/Monkey_patch">monkey patch</a>, or just fork and patch them up (don't forget to open a pull request later!).
</p>
<p>
Also if you're the author of bindings, I have some requests. Please:
</p>
<ul class="org-ul">
<li>don't print in stdout, it's a pain to filter out and suppress. Ideally use proper logging modules</li>
<li><p>
don't be overly defensive, or allow to <a href="mypy-error-handling.html#global_policy">configure</a> non-defensive behavior
</p>
<p>
It's quite sad when the library silently catches all exceptions and replaces with empty strings/nulls/etc., without you even suspecting it.
It's especially problematic in Python, where "Ask forgiveness, not permission" is very common.
</p></li>
<li><p>
expose raw underlying data (e.g. raw JSON/XML from the API)
</p>
<p>
If you forget to handle something, or the user disagrees with the interpretation of data, they would still be able to benefit from the data bindings for retrieval and only alter the deserialization.
</p>
<p>
Example of good data object:
</p>
<ul class="org-ul">
<li><a href="https://github.com/pawelad/pymonzo/blob/b5c8d4f46dcb3a2f475797a8b8ef1c15f6493fb9/src/pymonzo/api_objects.py#L38-L45">pymonzo</a> exposes programmer-friendly fields and also keeps raw data</li>
</ul></li>
<li><p>
expose generic methods for handling API calls to make it easy to add new endpoints
</p>
<p>
Same argument: if you forgot to handle some API calls, it makes it much easier for consumers to quickly add them.
</p></li>
</ul>
</div>
<div class="outline-4" id="outline-container-org0000013">
<h4 id="org0000013">examples</h4>
<div class="outline-text-4" id="text-org0000013">
<p>
To export <a href="https://github.com/karlicoss/hypexport">Hypothes.is</a> data I'm using existing <a href="https://github.com/judell/Hypothesis">judell/Hypothesis</a> bindings.
</p>
<ul class="org-ul">
<li>the bindings handle <a href="https://github.com/judell/Hypothesis/blob/91f881693546aaddc4096327a97f5cf342c3770a/hypothesis.py#L69">pagination and rate limits</a> for you</li>
<li>the bindings return raw JSONs, making it trivial to serialize the data on disk</li>
</ul>
<ul class="org-ul">
<li><p>
the bindings expose generic <a href="https://github.com/judell/Hypothesis/blob/91f881693546aaddc4096327a97f5cf342c3770a/hypothesis.py#L138"><code class="inline">authenticated_api_query</code></a> method
</p>
<p>
For instance, profile data request was missing from the bindings; and it was <a href="https://github.com/karlicoss/hypexport/blob/7a80b36aa55da8b541e2778141eb84ada384d734/hypexport.py#L14">trivial</a> to get it anyway 
</p></li>
</ul>
<p>
Thanks to good bindings, the actual export is pretty much <a href="https://github.com/karlicoss/hypexport/blob/7a80b36aa55da8b541e2778141eb84ada384d734/hypexport.py#L6-L19">trivial</a>.
</p>
<p>
Another example: to export <a href="https://github.com/karlicoss/rexport/blob/master/export.py">Reddit data</a>, I'm using <a href="https://github.com/praw-dev/praw">praw</a>, an excellent library for accessing Reddit from Python.
</p>
<ul class="org-ul">
<li>praw handles rate limits and pagination</li>
<li>praw exposes logger, which makes it easy to <a href="https://github.com/karlicoss/rexport/blob/874e6116bfba8cbd63fa3b4d93810a1488cb8464/export.py#L107">control it</a></li>
<li>praw supports all endpoints, so exporting data is just a matter of <a href="https://github.com/karlicoss/rexport/blob/d001e2d07d716130106ebe07a021f98d84a5ed93/rexport.py#L73-L84">calling right API methods</a></li>
<li><p>
one shortcoming of praw though is that it wouldn't give you access to raw JSON data for some reason, so we have to use some <a href="https://github.com/karlicoss/rexport/blob/d001e2d07d716130106ebe07a021f98d84a5ed93/rexport.py#L32-L55">hacky logic</a> to serialize.
</p>
<p>
If praw kept original data from the API, the <a href="https://github.com/karlicoss/rexport/blob/master/export.py">code for export</a> would be twice as short.
</p></li>
</ul>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org0000015">
<h3 id="keep_raw"><a class="headerlink" href="#keep_raw">¶</a>don't mess with the raw data</h3>
<div class="outline-text-3" id="text-keep_raw">
<p>
Keep the data you retrieved <b>as intact as possible</b>.
</p>
<p>
That means:
</p>
<ul class="org-ul">
<li>don't insert it in in a database, unless it's really necessary</li>
<li>don't convert formats (e.g. JSON to XML)</li>
<li>don't try to clean up and normalize</li>
</ul>
<p>
Instead, <b>keep the exporter code simple</b> and don't try to interpret data in it.
Move data interpretation burden to the <a class="link-down" href="#dal">data access layer</a> instead.
</p>
<p>
The main rationale for is is that it's a potential source of inconsistencies. If you make a bug during data conversion, you might end corrupt your data forever.
</p>
<p>
I'm elaborating on this point <a href="unnecessary-db.html#asis">here</a>.
</p>
</div>
</div>
<div class="outline-3" id="outline-container-org0000016">
<h3 id="defensive"><a class="headerlink" href="#defensive">¶</a>don't  be too defensive</h3>
<div class="outline-text-3" id="text-defensive">
<ul class="org-ul">
<li>never silently fallback on default values in case of errors, unless you're really certain about what you're doing</li>
<li><p>
don't add retry logic just in case
</p>
<p>
In my experience, it's fair to assume that if the export failed, it's a random server-side glitch and not worth fine-tuning, it's easier to simply start the export all over again.
I'm not dealing with that in the individual export scripts at all, and using <a class="link-down" href="#arctee">arctee</a> instead, to retry exports automatically.
</p>
<p>
If you know what you're doing (e.g. some endpoint is notoriously flaky) and do need retries, I recommend using an existing library that handles that, like <a href="https://github.com/litl/backoff#examples">backoff</a>.
</p></li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0000017">
<h3 id="credentials"><a class="headerlink" href="#credentials">¶</a>allow reading credentials from a file</h3>
<div class="outline-text-3" id="text-credentials">
<ul class="org-ul">
<li>you don't want them in your shell history or in crontabs</li>
<li><p>
keeping them in a file can potentially allow for fine access control
</p>
<p>
E.g. with Unix permissions you could only allow certain scripts to read secrets.
Note that I'm not a security expert, so would be interested to know if there are better solutions to that
</p>
<p>
Personally, I found it so boilerplaty, that I extracted this logic in a separate <a href="https://github.com/karlicoss/instapexport/blob/master/export_helper.py">helper module</a>. You can find an example <a href="https://github.com/karlicoss/instapexport#exporting">here</a>.
</p></li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org000001d">
<h2 id="storage"><a class="headerlink" href="#storage">¶</a><span class="section-number-2">5</span> How to store it: organizing data</h2>
<div class="outline-text-2" id="text-storage">
<p>
As I mentioned, for the most part, I'm just keeping the raw API data.
For the storage I'm just using the filesystem; all exports are kept or symlinked in the same directory (<code class="inline">/exports</code>) for the ease of access:
</p>
<div class="org-src-container">
<pre class="src src-bash">find /exports/ | sort | head -n 20 | tail -n 7
</pre>
</div>
<pre class="example">
/exports/feedbin
/exports/feedly
/exports/firefox-history
/exports/fitbit
/exports/github
/exports/github-events
/exports/goodreads
</pre>
</div>
<div class="outline-3" id="outline-container-org0000019">
<h3 id="timestamps"><a class="headerlink" href="#timestamps">¶</a>naming and timestamping</h3>
<div class="outline-text-3" id="text-timestamps">
<p>
The only important bit is making sure that if you keep multiple exports (e.g. <a class="link-up" href="#synthetic">synthetic</a>), their names include timestamps, and time order is consistent with lexicographic order.
</p>
<p>
That means that that the only acceptable date/time format is some variation of <a href="https://en.wikipedia.org/wiki/ISO_8601"><code class="inline">YYYY MM DD HH MM SS Z</code></a>. 
Feel free to sprinkle it with any separators you like, or use milliseconds if you are really serious, but any other date format, e.g. <samp class="inline">MM/DD/YY</samp>, using month names, or not using zero-padded numbers is going to bring you serious grief.
</p>
<p>
E.g.:
</p>
<div class="org-src-container">
<pre class="src src-bash">ls /exports/instapaper/ | tail -n 5
</pre>
</div>
<pre class="example">
instapaper_20200101T000005Z.json
instapaper_20200101T040004Z.json
instapaper_20200101T080010Z.json
instapaper_20200101T120005Z.json
instapaper_20200101T160011Z.json
</pre>
<p>
The reason is it's automatically sort/max friendly, which massively reduces the cognitive load when working with data.
</p>
<p>
To make timestamping automatic and less boilerplaty, I'm using a <a class="link-down" href="#arctee">wrapper script</a>.
</p>
</div>
</div>
<div class="outline-3" id="outline-container-org000001a">
<h3 id="backups"><a class="headerlink" href="#backups">¶</a>backups</h3>
<div class="outline-text-3" id="text-backups">
<p>
Backups are trivial: I can just run <a href="https://borgbackup.readthedocs.io/en/stable">borg</a> against <samp class="inline">/exports</samp>.
What is more, borg is deduplicating, so it's very friendly to incremental and synthetic exports.
</p>
</div>
</div>
<div class="outline-3" id="outline-container-org000001b">
<h3 id="sync"><a class="headerlink" href="#sync">¶</a>synchronizing between computers</h3>
<div class="outline-text-3" id="text-sync">
<p>
I synchronize/replicate it across my computers with Syncthing, also used Dropbox in the past.
</p>
</div>
</div>
<div class="outline-3" id="outline-container-org000001c">
<h3 id="disk_space"><a class="headerlink" href="#disk_space">¶</a>disk space concerns</h3>
<div class="outline-text-3" id="text-disk_space">
<p>
A back on the envelope argument that it shouldn't be a concern to you:
</p>
<ul class="org-ul">
<li>the amount of data you generate grows linearly. That means that running exports periodically would take 'quadratic' space</li>
<li>with time, the storage available to you grows exponentially (and only gets cheaper)</li>
</ul>
<p>
Hopefully, that's convincing, but if this is an issue, it can also be addressed with compression or even using deduplicating backup software like <a href="https://borgbackup.readthedocs.io/en/stable">borg</a> for storage. However, that would come at the cost of slowing down access.
Although it might not be an issue if you use caching.
</p>
<p>
For most of my exports, I don't even bother compressing, for the few that I do need to compress, <a class="link-down" href="#arctee">arctee wrapper</a> handles it.
Remember that if you compress, you will typically be paying for it with the ease and the speed of data access.
</p>
<p>
There are potential ways of making it transparent:
</p>
<ul class="org-ul">
<li><p>
keeping data under borg and using <a href="https://borgbackup.readthedocs.io/en/stable/usage/mount.html">borg mount</a> to access it.
</p>
<p>
You get deduplication for free, however, this makes exporting and accessing data way more obscure, and in addition borg mount locks the repository so it's going to be read-only while you access it.
</p></li>
<li><p>
using filesystem capable of compressing on the fly
</p>
<p>
E.g. <a href="https://serverfault.com/questions/740456/lightweight-transparent-compression-filesystem">ZFS/BTRFS</a>.
</p>
<p>
It seems straightforward enough, as non-standard file systems moth be incompatible with some software, e.g. <a href="https://www.linuxuprising.com/2018/11/how-to-use-dropbox-on-non-ext4.html">Dropbox</a>.
I haven't personally tried it.
</p></li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0000020">
<h2 id="dal"><a class="headerlink" href="#dal">¶</a><span class="section-number-2">6</span> Data access layer (DAL)</h2>
<div class="outline-text-2" id="text-dal">
<p>
As I <a class="link-up" href="#design">mentioned</a>, all that DAL does is maps raw data (saved on the disk by the <a class="link-up" href="#export_layer">export layer</a>) onto abstract objects, making it easier to work with in your programs. "Layer" sounds a bit intimidating, but in reality, it's usually just a single short script.
</p>
<p>
It's meant to deal with data cleanup, normalization, etc. Doing this in runtime rather than during export makes it easier to work around data issues, experiment, and forgiving if you make some bugs.
</p>
<p>
  As I mentioned in the <a class="link-up" href="#design">design principles</a>, I'm trying to keep data retrieval code and data access code separate,
because they serve very different purposes and deal with very different errors.
</p>
<p>
Just as a reminder what we get as a result:
</p>
<ul class="org-ul">
<li><p>
resilience
</p>
<p>
Accessing and working with data on your disk is considerably easier and faster than using APIs.
</p></li>
<li><p>
offline
</p>
<p>
You only access data on your disk, which makes you completely independent on the Internet.
</p></li>
<li><p>
modularity and decoupling: you can use separate tools (even written in different programming languages) for retrieving and accessing data
</p>
<p>
That's very important, so we all can benefit from existing code and reinventing less wheels.
</p></li>
<li><p>
backups
</p>
<p>
Keeping raw data makes them trivial
</p></li>
</ul>
</div>
<div class="outline-3" id="outline-container-org000001e">
<h3 id="dal_performance"><a class="headerlink" href="#dal_performance">¶</a>performance concerns</h3>
<div class="outline-text-3" id="text-dal_performance">
<p>
A natural question is: if you run through all your data snapshots each time you access it, wouldn't it bee too slow?
</p>
<p>
First of all, it's somewhat similar to the worries about the <a class="link-up" href="#disk_space">disk space</a>. Data grows at the quadratic rate; while processing power doesn't seem to follow Moore's law anymore, there is still some potential to scale horizontally and use multiple threads. In practice, for most data sources that I use, this process is almost instantaneous even without parallelizing anyway.
</p>
<p>
In addition:
</p>
<ul class="org-ul">
<li>if you're using iterators/generators/coroutines (e.g. <a href="https://github.com/karlicoss/rexport/blob/874e6116bfba8cbd63fa3b4d93810a1488cb8464/dal.py#L130-L136">example</a>), that overhead will be amortized and basically unnoticeable</li>
<li>you can still use caching, just make sure it doesn't involve boilerplate or cognitive overhead to use. E.g. <a href="unnecessary-db.html#cachew_cachew">cachew</a>.</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org000001f">
<h3 id="dal_examples"><a class="headerlink" href="#dal_examples">¶</a>examples</h3>
<div class="outline-text-3" id="text-dal_examples">
<div class="noop" id="dal_messenger">
<p>
Example: <a href="https://github.com/karlicoss/fbmessengerexport/blob/a8f65a259dfa36ab6d175461994356947ded142a/model.py#L27-L47">DAL for Facebook Messenger</a> knows how to read messages from the database, access certain fields (e.g. message body) and handle obscure details like converting timestamps to <samp class="inline">datetime</samp> objects. 
</p>
<ul class="org-ul">
<li>it's <b>not</b> trying to get messages from Facebook, which makes it way faster and more reliable to interact with data.</li>
<li>it's <b>not</b> trying to do anything fancier past providing access to the data, which allows to keep it simple and resilient</li>
</ul>
</div>
<p>
You can find more specific examples along with the motivation and explanations here:
</p>
<ul class="org-ul">
<li><a href="unnecessary-db.html#example_reddit">Reddit</a></li>
<li><a href="unnecessary-db.html#relational">Instapaper/Endomondo</a></li>
<li><a href="unnecessary-db.html#maintaining">Pocket</a></li>
<li><a href="unnecessary-db.html#example_chrome">Chrome</a></li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0000023">
<h2 id="automatic_exports"><a class="headerlink" href="#automatic_exports">¶</a><span class="section-number-2">7</span> Automating exports</h2>
<div class="outline-text-2" id="text-automatic_exports">
<p>
I think it's absolutely essential to automate data exports when possible. 
You really don't want to think about it, and having a recent version of your data motivates you to actually use it, otherwise, there is much less utility.
</p>
<p>
In addition, it serves as a means of backup, so you don't have to worry about what happens if the service ceases to exist.
</p>
</div>
<div class="outline-3" id="outline-container-org0000021">
<h3 id="scheduling"><a class="headerlink" href="#scheduling">¶</a>scheduling</h3>
<div class="outline-text-3" id="text-scheduling">
<p>
I run most of my data exports at least daily.
</p>
<p>
I wrote a whole <a href="scheduler.html">post</a> on scheduling and job running with respect to the personal infrastructure. In short:
</p>
<ul class="org-ul">
<li><p>
on desktop: at the moment, I'm mostly using cron (to be more specific, <a href="scheduler.html#fcron">fcron</a>).
</p>
<p>
I'm still <a href="scheduler.html#solution">thinking</a> of an alternative, but overall using cron is okay.
</p></li>
<li>on Android phone: <a href="scheduler.html#phone">I'm using Automate app and cron</a></li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0000022">
<h3 id="arctee"><a class="headerlink" href="#arctee">¶</a>arctee</h3>
<div class="outline-text-3" id="text-arctee">
<p>
This is a <a href="https://github.com/karlicoss/arctee">wrapper script</a> I'm using to run most of my data exports.
</p>
<p>
Many things are very common to all data exports, regardless of the source.
In the vast majority of cases, you want to fetch some data, save it in a file (e.g. JSON) along with a timestamp and potentially compress.
</p>
<p>
This script aims to minimize the common boilerplate:
</p>
<ul class="org-ul">
<li><samp class="inline">path</samp> argument allows easy ISO8601 timestamping and guarantees atomic writing, so you'd never end up with corrupted exports.</li>
<li><samp class="inline">--compression</samp> allows to compress simply by passing the extension. No more <samp class="inline">tar -zcvf</samp>!</li>
<li><samp class="inline">--retries</samp> allows easy exponential backoff in case service you're querying is flaky.</li>
</ul>
<p>
Example:
</p>
<pre class="example">
arctee '/exports/rtm/{utcnow}.ical.zstd' --compression zstd --retries 3 -- /soft/export/rememberthemilk.py
</pre>
<ol class="org-ol">
<li><p>
runs <samp class="inline">/soft/export/rememberthemilk.py</samp>, retrying it up to three times if it fails
</p>
<p>
The script is expected to dump its result in stdout; stderr is simply passed through.
</p></li>
<li>once the data is fetched it's compressed as <samp class="inline">zstd</samp></li>
<li>timestamp is computed and compressed data is written to <samp class="inline">/exports/rtm/20200102T170015Z.ical.zstd</samp></li>
</ol>
<p>
The wrapper operates on files, hence it's programming language agnostic, as long as you export script simply outputs to stdout (or accepts a filename, so you can use <samp class="inline">/dev/stdout</samp>), it doesn't really matter how (e.g. which programming language) it's implemented.
</p>
<p>
That said, it feels kind of wrong to have an extra script for all these things since they are not hard in principle, just tedious and boring to do all over again. If anyone has bright ideas on simplifying this, I'd be happy to know!
</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0000024">
<h2 id="fin"><a class="headerlink" href="#fin">¶</a><span class="section-number-2">8</span> --</h2>
<div class="outline-text-2" id="text-fin">
<p>
Approaches that I described here worked pretty well for me so far. It feels fairly composable, flexible and easy to maintain.
</p>
<p>
I'm sharing this because I would <b>really</b> like to make it accessible to more people, so they can also benefit from using their data.
</p>
<p>
I'd be happy to hear any suggestions on simplifying and improving the system!
</p>
</div>
</div>

    </section>

    
    <section class="footer">
        <div class="post-tags"><a class="post-tag" href="./tags.html#infra">#infra</a> <a class="post-tag" href="./tags.html#dataliberation">#dataliberation</a> <a class="post-tag" href="./tags.html#pkm">#pkm</a></div>
        <!-- TODO post-date? -->
        <div class="date">09 February 2020</div>
    </section>
    

    

    <section class="comments">
    <script data-isso="https://beepb00p.xyz/comments/" data-isso-reply-to-self="true" src="https://beepb00p.xyz/comments/js/embed.min.js">
</script>

<section id="isso-thread" data-isso-id="isso_exports"></section>

    </section>

</article>

        </main>

        <!-- TODO hmm maybe display something in a footer, so it's clear it's end of content... -->
        

        <!-- TODO make semantic -->
        <footer>
            <span style="float:left">
            <a href="https://twitter.com/karlicoss">🐦 me @twitter</a>
            ·
            <a href="https://github.com/karlicoss">💻 me @github</a>
            </span>

            <a href="http://creativecommons.org/licenses/by/4.0">CC BY 4.0</a>
            
            
        </footer>
    </body>
</html>
